{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9e70690",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/leolorenzoii/ml2_interpretability/blob/main/notebooks/01_Model_Interpretability_and_Shapley_Values.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" align=\"left\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996bc83a",
   "metadata": {},
   "source": [
    "# Model Interpretability and Shapley Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44863d11",
   "metadata": {},
   "source": [
    "Throughout the **Machine Learning 1** course, we have learned all about different machine learning algorithms and how they work and tune them. In a nutshell, we have been able to train a machine learning model and use it to make predictions on unseen data (see Figure <a href='#fig:ml-nutshell'>1</a>).\n",
    "\n",
    "<a name='fig:ml-nutshell'></a>\n",
    "<div>\n",
    "<img src=\"images/ml-nutshell.png\" align=\"left\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "<br style=\"clear:both\" />\n",
    "\n",
    "<div>\n",
    "    <p style=\"font-size:12px;font-style:default;\">\n",
    "        <b>Figure 1. Training and testing a machine learning model in a nutshell.</b><br>\n",
    "        We learned how to train a machine learning model and use it to predict unseen data.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "<br style=\"clear:both\" />\n",
    "\n",
    "However, when it comes to choosing the optimal model, often the most accurate ones are the least interpretable (see Figure <a href='#fig:accuracy-interpretability'>2</a>) [[1]](#ref:interpret-ml).\n",
    "\n",
    "<a name='fig:accuracy-interpretability'></a>\n",
    "<div>\n",
    "<img src=\"images/accuracy-interpretability-trade-off.PNG\" align=\"left\" width=\"450\"/>\n",
    "</div>\n",
    "\n",
    "<br style=\"clear:both\" />\n",
    "\n",
    "<div>\n",
    "    <p style=\"font-size:12px;font-style:default;\">\n",
    "        <b>Figure 2. Machine learning model accuracy and interpretability tradeoff.</b><br>\n",
    "           Models that are highly accurate are the least interpretable, while models that are highly interpretable have a sub-par accuracy [<a href='#ref:interpret-ml'>1</a>].\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "Linear models offer the highest interpretability but offer sub-par accuracy. Tree-based ensemble models such as Random Forest and Gradient Boosted trees offer top accuracy but interpretation is limited to knowing the most important features used by the model.\n",
    "\n",
    "We are left with several unanswered questions such as:\n",
    "\n",
    "- How does one (or more) feature impact the predictions of the model?\n",
    "- What is the role that each feature value play in each individual predictions?\n",
    "- How can we explain the models predictions in a more useful manner for our stakeholders?\n",
    "\n",
    "We will deal with such questions in this notebook. In particular, we will discuss how we can interpret better our models predictions with focus on the concept of **Shapley Values**. We will first enrich our background by discussing core concepts of model interpretability then discuss Shapley values and how we can use it to interpret our model's prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eff86f",
   "metadata": {},
   "source": [
    "## What is model interpretability?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ad8798",
   "metadata": {},
   "source": [
    "In Christoph Molnar's [Interpretable Machine Learning](#ref:molnar) book [[2]](#ref:molnar), he collated two definitions of **interpretability**. A non-mathematical one:\n",
    "\n",
    "> *Interpretability is the degree to which a* ***human*** *can* ***understand*** *the cause of a* ***decision***. [[3]](#ref:miller)\n",
    "\n",
    "And a mathematical one:\n",
    "\n",
    "> *Interpretability is the degree to which a* ***human*** *can consistently* ***predict*** *the* ***modelâ€™s result***. [[4]](#ref:kim)\n",
    "\n",
    "In both definitions we see three important elements: the **human**, the **understanding**, and the **decision**. Thus, in the same way, when we construct our definition for *interpretability* in the context of machine learning:\n",
    "\n",
    "> ***Model interpretability*** *refers to the degree in which the behaviors and predictions of machine learning models are understandable to humans.*\n",
    "\n",
    "Notice here that there is a *flexibility* in the definition of model interpretability. Indeed, defining how explicable a machine learning model is depends on the needs and requirements of a project and the different stakeholders.\n",
    "\n",
    "Nevertheless, as ethical machine learning practitioners and data science leaders, model interpretability MUST be integrated as early as possible in the development process and should NOT just be taken as an afterthought. Following the CRISP-DM framework [[5]](#ref:crisp-dm), model interpretability requirements must be deliberated during the first phase of the project and implemented during model evaluation and deployment (see Figure [3](#fig:crisp-dm)).\n",
    "\n",
    "<a name='fig:crisp-dm'></a>\n",
    "<div>\n",
    "<img src=\"images/crisp-dm-interpretability.png\" align=\"left\" width=\"450\"/>\n",
    "</div>\n",
    "\n",
    "<br style=\"clear:both\" />\n",
    "\n",
    "<div>\n",
    "    <p style=\"font-size:12px;font-style:default;\">\n",
    "        <b>Figure 3. Model interpretability integrated in the CRISP-DM machine learning project lifecycle.</b><br>\n",
    "           Model interpretability requirements must be deliberated during the first phase of the project and be implemented during model evaluation and deployment.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275578aa",
   "metadata": {},
   "source": [
    "### Why interpretability?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba0b6d2",
   "metadata": {},
   "source": [
    "Now, one might ask, why bother with interpretability? Wouldn't a high model performance would ultimately yield to higher business value? This is quite a arguable topic and in fact, in 2017, the [Neural Information Processing Systems](https://nips.cc/) conference in 2017 had its first ever ***The Great AI Debate*** with the topic ***Is interpretability necessary for machine learning?*** [[6]](#ref:great-ai-debate) *(you are highly encourage to watch this engaging and insightful discussion* ðŸ™‚).\n",
    "\n",
    "In the video, it was shown that model interpretability is crucial especially for some applications where quirky patterns from the data may be learned by the model. Indeed, having an interpretability pipeline in your project gives you, as the Data Scientist the ability to debug your model and identify issues early on, thus, giving you a chance to improve your model. Furthermore, it has added benefits for other stakeholders that is interested and affected by your machine learning model (see Figure [4](#fig:stakeholders)).\n",
    "\n",
    "<a name='fig:stakeholders'></a>\n",
    "<div>\n",
    "<img src=\"images/interpretability-stakeholders.png\" align=\"left\" width=\"700\"/>\n",
    "</div>\n",
    "\n",
    "<br style=\"clear:both\" />\n",
    "<br style=\"clear:both\" />\n",
    "\n",
    "<div>\n",
    "    <p style=\"font-size:12px;font-style:default;\">\n",
    "        <b>Figure 4. Benefits of model interpretability to various stakeholders of a machine learning project.</b><br>\n",
    "           Model interpretability benefits the data scientist, business decision makers, approving authorities, and business customers.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "For **data scientists**, being able to explain the model to other stakeholders is also one benefit of having model explicability. The better you can explain the model to other people in the business, the greater its chance of being adopted and the trust given to the model by other stakeholders. With model interpretability, **business executives** now has the option to provide transparency to its end-users. Furthermore, it helps them justify the business case for the investment and identify other potential extensions and business use-case for the project. **Approving authorities** also benefit from model interpretability, by having a clear understanding of the risk the business is going to take in adopting the model, understanding the impact of the model decisions to humans, and anticipate any legal or regulatory issues that the model may face. Finally, **customer** experience and decision making can also be improved if they understand why a model gives a certain prediction.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Points for Discussion**\n",
    "\n",
    "Here, we outlined the benefits of having an interpretability pipeline in our machine learning projects. Can you think of cases where having model interpretability is NOT preferred? Give a particular instance where having model interpretability can do more harm than good.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33556941",
   "metadata": {},
   "source": [
    "### What makes a good explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13dfe86",
   "metadata": {},
   "source": [
    "Before we begin explaining predictions of machine learning models, it helps to understand what makes an explanation good an acceptable for humans. We are making explanations, after all, for a humans to be digested. This help us better frame the model explanations we get from model interpretability methods.\n",
    "\n",
    "Here are some of the few important characteristics of a good explanation *(see Chapter 3.6 of Molnar's Interpretable Machine Learning for a complete list [[2]](#ref:molnar))*:\n",
    "\n",
    "1. **Contrastive** - Model explanations must be able to answer *why a given prediction has been made in place of another prediction*. For example, in a model that recommends whether an individual be given a loan or not, a good explanation must be able to tell *what factors should/could I change to alter the model prediction*.\n",
    "2. **Selective** - We *humans are only capable to comprehend 2 to 3 variables at a time*. Thus, a good model explanation must be able to *list the important drivers to explain an outcome*. Imagine having an interpretable model such as linear regression or decision trees, but an explanation that looks at hundreds or thousands of variables, it would be really hard for any human to digest that explanation!\n",
    "3. **Consistent with prior beliefs** - Model explanations are greatly affected by how people perceived them. As such, when *an explanation is consistent with the prior beliefs of an individual*, they tend to favor such explanation ( also known as **confirmation bias**). This is not to discredit any novel or serendipitous discoveries of the model explanation. However, having an explanation that is in line with a domain expert, helps in the model's adoption. Furthermore, if a model is found to exhibit behavior inconsistent with the domain expert's belief, we can enforce constraints on the model or use a linear model that has the required property.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Points for Discussion**\n",
    "\n",
    "Among the interpretability methods that you currently know, can you create model explanations that satisfies all of the three characteristics we discussed above?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb1295d",
   "metadata": {},
   "source": [
    "### Types of Explainability Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc8a02b",
   "metadata": {},
   "source": [
    "Model interpretability methods can be classified according to three different criteria (see Figure 5) [[7]](#ref:bbox-peek):\n",
    "\n",
    "<a name='fig:taxonomy'></a>\n",
    "<div>\n",
    "<img src=\"images/taxonomy.png\" align=\"left\" width=\"550\"/>\n",
    "</div>\n",
    "\n",
    "<br style=\"clear:both\" />\n",
    "<br style=\"clear:both\" />\n",
    "\n",
    "<div>\n",
    "    <p style=\"font-size:12px;font-style:default;\">\n",
    "        <b>Figure 5. Taxonomy of different explainability methods.</b><br>\n",
    "           Model interpretability or explainability methods can be intrinsic or post-hoc, model-specific or model-agnostic, local or global. Model-agnostic methods can be further classified whether they use surrogate models or are just visualizations of the behavior of the black-box model.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "First, we can classify whether the method stems from the model being **intrinsically** interpretable. If the model is not intrinsically interpretable, then the explanation method can be applied **post-hoc** or post-model training. Examples of intrinsically interpretable models include: Linear models, Decision Tree, and Rule-based models ([RuleFit](https://github.com/rohan-gt/rulefit) [[8]](#ref:rulefit)).\n",
    "\n",
    "We can further classify the method whether it is **model-specific** or **model-agnostic** (intrinsic explainability methods are model-specific by definition). Model agnostic means that the explainability method can be applied to any black box models. They can be further divided into whether they apply **surrogate** models (e.g., LIME or SHAP Kernel) or is a **visualization** of the behavior of the black box model (e.g., Partial Dependence Plots, Individual Conditional Expectations, or Accumulated Local Effects) [[9]](#ref:med-image).\n",
    "\n",
    "Finally, we can classify an explainability method whether it is a **global** explanation - i.e., it explains the whole model behavior (ex. feature importance and summary visualizations), or whether it is a **local** explanation, i.e., it explains a particular instance in the test or train dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff084fec",
   "metadata": {},
   "source": [
    "## Shapley Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c963432",
   "metadata": {},
   "source": [
    "The **Shapley values** is a hallmark concept in the field of cooperative game-theory. As applied to machine learning model interpretability, it gives a key insights in defining feature importance and the roles each feature play in a prediction. *The Shapley value is the only explanation method grounded on a* ***solid theory*** and ensures that difference between the *average prediction* is fairly distributed among the feature values of the instance. This is one key advantage of Shapley values, and in situations where regulations or law require explainability, Shapley might be the only legal compliant explainability method [[2]](#ref:molnar).\n",
    "\n",
    "It is by nature a local explanation method, but due to its characteristic, it can also be used for global explanations. The contrastive explanations it provides, coupled with its flexibility and practicality, makes explanaibility methods based from Shapley values highly popular. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919f0b55",
   "metadata": {},
   "source": [
    "### Definition and Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dca23fa",
   "metadata": {},
   "source": [
    "Shapley values is a solution in cooperative game-theory answering how to distribute a **gain** of a **coalition** *fairly* by considering the outcome of each possible combination (or coalition) of **players** to determine the importance of each player. The distribution is based on the **expected marginal contribution** of a player when it joins the coalition (at every possible ordering).\n",
    "\n",
    "To understand the significance of Shapley values and how it is computed, let us consider the following demonstration [[10]](#ref:shap-how):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69c0501",
   "metadata": {},
   "source": [
    "Say that in a certain RPG game, a Dragon Quest was offered by a certain dungeon master:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b2a34f",
   "metadata": {},
   "source": [
    "<img src=\"images/dragon_quest/1.png\" align=\"center\" width=\"250\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6aafff1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T17:43:52.375545Z",
     "start_time": "2021-10-11T17:43:52.231546Z"
    }
   },
   "source": [
    "*\\\"For every dragon tamed, your team gets a thousand gold coins\\\"* the Dungeon Master said.\n",
    "\n",
    "Heeding the call of this dragon quest are the band of brave adventurers: The (1) Fiesty Sorcerres, (2) Mighty Knight, and the (3) Keen Archer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b372af",
   "metadata": {},
   "source": [
    "<img src=\"images/dragon_quest/2.png\" align=\"center\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34c3c98",
   "metadata": {},
   "source": [
    "For the duration of the quest, the three tamed *30 dragons* granting them a total bounty of *30 thousand gold*!\n",
    "\n",
    "Initially, the team thought of distributing the bounty *evenly*, granting each member a take-home bounty of *10 thousand gold* each. However, the Fiesty Sorceress reacted: *\\\"It is my spells that weaken the dragons that we encountered, I think it is not fair that I don't get a bonus in the bounty that we have!\\\"*\n",
    "\n",
    "This presents a dillema, since giving the Fiesty Sorceress a bonus means cutting the share of some of the other members. The question now is: ***Is there any truth to the claim of the Fiesty Sorceress? How should we distribute the bounty fairly among the members of the team?***\n",
    "\n",
    "Here is where the Shapley values becomes useful. By considering the *expected marginal contribution* of each member as they enter the coalition in all possible ordering, we can compute for the amount of bounty that each player should receive.\n",
    "\n",
    "As a demonstration, let's compute the Shapley value of the Fiesty Sorceress and see if there's any truth to her claim."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa490a0",
   "metadata": {},
   "source": [
    "<img src=\"images/dragon_quest/3.png\" align=\"center\" width=\"450\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69568c8c",
   "metadata": {},
   "source": [
    "The Fiesty Sorceress can enter the coalition in four different scenarios: (1) Coming from **No Members** $\\varnothing$ to a single player (Fiesty Sorceress); (2) Joining the **Brave Knight** forming a party of 2 (Brave Knight, Fiesty Sorceress); (3) Joining the **Keen Archer** forming a party of 2 (Keen Archer, Fiesty Sorceress); and (4) Joining the team of **Brave Knight and Keen Archer** forming a party of 3 (Brave Knight, Keen Archer, Fiesty Sorceress).\n",
    "\n",
    "We then compute the **marginal contribution** of the Fiesty Sorceress as she enters the coalition from these four scenarios:\n",
    "\n",
    "1. $\\varnothing$ to **(Fiesty Sorceress)**: 10 dragons - 0 dragons = **10 dragons**\n",
    "2. **(Brave Knight)** to **(Brave Knight, Fiesty Sorceress)**: 20 dragons - 5 dragons = **15 dragons**\n",
    "3. **(Keen Archer)** to **(Keen Archer, Fiesty Sorceress)**: 16 dragons - 8 dragons = **8 dragons**\n",
    "4. **(Brave Knight, Keen Archer)** to **(Brave Knight, Keen Archer, Fiesty Sorceress)**: 30 dragons - 18 dragons = **12 dragons**\n",
    "\n",
    "To compute for the **expected marginal contribution** of the Fiesty Sorceress (or equivalently, her Shapley value $\\phi_S$), we have to weight these marginal contributions according to each scenario:\n",
    "\n",
    "$$\n",
    "\\phi_S = 10 \\times w_1 + 15 \\times w_2 + 8 \\times w_3 + 12 \\times w_4 \\tag{1}\n",
    "$$\n",
    "\n",
    "Now, how do we determine the values of the weights $w_1$, $w_2$, $w_3$, and $w_4$? There are three key ideas:\n",
    "\n",
    "1. Since we are computing for the **expected** marginal contribution, the sum of all the weights should reduce to unity:\n",
    "\n",
    "$$\n",
    "w_1 + w_2 + w_3 + w_4 = 1 \\tag{2} \\label{eq:marginal-1}\n",
    "$$\n",
    "\n",
    "2. Next, we impose that weights $w_2$ and $w_3$ should have the same value. Since, in both scenarios, the Fiesty Sorceress enters the coalition from a party of one.\n",
    "\n",
    "$$\n",
    "w_2 = w_3 \\tag{3} \\label{eq:marginal-2}\n",
    "$$\n",
    "\n",
    "3. Finally, notice that the Fiery Sorceress enters at three distinct scenario: (1) from no player, (2) from a party of one, and (3) from a party of two. We impose that the weights resulting from these scenarios should be equivalent, that is:\n",
    "\n",
    "$$\n",
    "w_1 = w_2 + w_3 = w_4 \\tag{4} \\label{eq:marginal-3}\n",
    "$$\n",
    "\n",
    "Combining Equations \\ref{eq:marginal-1}, \\ref{eq:marginal-2}, \\ref{eq:marginal-3}, we get:\n",
    "\n",
    "$$\n",
    "w_1 = \\frac{1}{3} \\\\\n",
    "w_2 = w_3 = \\frac{1}{6}  \\\\\n",
    "w_4 = \\frac{1}{3} \\tag{5}\n",
    "$$\n",
    "\n",
    "Thus, we can finally compute for the expected marginal contribution or Shapley value $\\phi_S$ of the Fiesty Sorceress:\n",
    "\n",
    "$$\n",
    "\\phi_S = 10 \\cdot \\frac{1}{3} + 15 \\cdot \\frac{1}{6} + 8 \\cdot \\frac{1}{6} + 12 \\cdot \\frac{1}{3} = 11 \\frac{1}{6} \\approx 11 \\tag{6}\n",
    "$$\n",
    "\n",
    "In the end, it seems that the Fiesty Sorceress provide some truth in her claim! Her Shapley value tell us that her contribution to the team is equivalent to *11 Dragons*, which means, she should get a total share of *11 thousand gold* of bounty!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305f4a83",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**Practice Exercise**\n",
    "\n",
    "Can you compute the Shapley values of the other team members (Brave Knight and Keen Archer)? Who gets a lower gold share compared to the average? How can we explain (and possibly argue) to that team member that they should get a smaller share of the gold?\n",
    "\n",
    "*Note: You may find the [Figure 6](#fig:dragon-quest) helpful in your computation. Grey arrows represent the Brave Knight joining the coalition, while green arrows represent the Keen Archer joining the coalition.*\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b653384a",
   "metadata": {},
   "source": [
    "<a name='fig:dragon-quest'></a>\n",
    "<div>\n",
    "    <img src=\"images/dragon_quest/4.png\" align=\"center\" width=\"450\"/>\n",
    "</div>\n",
    "\n",
    "<br style=\"clear:both\" />\n",
    "\n",
    "<div>\n",
    "    <p style=\"font-size:12px;font-style:default;\">\n",
    "        <b>Figure 6. Different possible coalitions and the corresponding weights and gains for each scenario.</b><br>\n",
    "           Each arrow represents a player entering the coalition. Grey arrows represent the Brave Knight joining the coalition, green arrows represent the Keen Archer, and the red arrows represent the Fiesty Sorceress.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462ca1a2",
   "metadata": {},
   "source": [
    "### Mathematical Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d2b35b",
   "metadata": {},
   "source": [
    "Now that we understand the significance of Shapley values and how they are computed in practice, we can proceed to derive a mathematical formula for it.\n",
    "\n",
    "Recall that by definition Shapley value is the **expected marginal contribution** of a player $i$ to the coalition it joins if the method of coalition formation is that each player enters one at a time [[11]](#ref:shapley-handbook). As such, for a given set of $N$ players, and for each non-empty subset $S$ of $N$, we can write the definition of Shapley value $\\phi_i$ for player $i$ in terms of probability as follows:\n",
    "\n",
    "$$\n",
    "\\phi_i = E(MC_i) = \\sum_{S \\subseteq N} P(S) MC_{i,S} \\tag{7} \\label{eq:shap-prob}\n",
    "$$\n",
    "\n",
    "Here, $P(S)$ represents the probability of $i$ joining the coalition to form $S$ and $MC_{i, S}$ is the marginal contribution as a results of $i$ joining the coalition.\n",
    "\n",
    "To calculate $P(S)$, let us consider the case when player $i$ joins the coalition to form set $S$ with $s$ members. Denoting $n$ as the total number of players, there are $n!$ possible orderings. To form the subset coalition $S$, note that other members of $S$ can enter before player $i$ in $(s-1)!$ permutations and the remaining players can enter in $(n-s)!$ permutations. Therefore, $P(S)$ may be given by:\n",
    "\n",
    "$$\n",
    "\\label{eq:prob-s} \\tag{8}\n",
    "$$\n",
    "<div>\n",
    "    <img src=\"images/prob-s.png\" align=\"center\" width=\"450\"/>\n",
    "</div>\n",
    "\n",
    "Furthermore, to formalize our definition, let $v(S)$ refer to the gain of the coalition subset $S \\subseteq N$ and $S \\, \\backslash \\, \\{ i \\}$ refer to the coalition subset $S$ but without player $i$. The marginal contribution of $i$ as it enters $S \\, \\backslash \\, \\{ i \\}$, $MC_{i, S}$, can therefore be written as:\n",
    "\n",
    "$$\n",
    "MC_{i, S} = v(S) - v(S \\, \\backslash \\, \\{ i \\}) \\tag{9} \\label{eq:mc-v}\n",
    "$$\n",
    "\n",
    "Combining equations \\ref{eq:shap-prob}, \\ref{eq:prob-s}, \\ref{eq:mc-v}, we get our **mathematical definition** of **Shapley values** as:\n",
    "\n",
    "$$\n",
    "\\label{eq:shapley-math} \\tag{10}\n",
    "$$\n",
    "<div>\n",
    "    <img src=\"images/shapley-math.png\" align=\"center\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Food for thought**\n",
    "\n",
    "You may have noticed that for every row in [Figure 6](#fig:dragon-quest), the fractions add up to one:\n",
    "\n",
    "1. A player enters the coalition from $\\varnothing$ to one member: $\\frac{1}{3} + \\frac{1}{3} + \\frac{1}{3} = 1$\n",
    "2. A player enters the coalition from one member to two members: $6 \\cdot \\frac{1}{6} = 1$\n",
    "3. A player enters the coalition from two members to three members: $\\frac{1}{3} + \\frac{1}{3} + \\frac{1}{3} = 1$\n",
    "\n",
    "Can you reconcile this result with our prior discussion wherein we define Shapley values in terms of probability?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9062612",
   "metadata": {},
   "source": [
    "## Shapley Values as an Explanation Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41552a92",
   "metadata": {},
   "source": [
    "Let's now discuss how **Shapley values** can be used as an **explanation method** to describe the predictions of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aef7eed",
   "metadata": {},
   "source": [
    "### Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981c9802",
   "metadata": {},
   "source": [
    "Recall that there are two key elements in cooperative game theory: **gain** and the **players**. When using Shapley values to explain machine learning models, we define the **gain** as the **outcome of the model** for a *single observation* and the **players** as **feature values** of that observation. A **coalition** would then correspond to a subset of feature values used in prediction. Thus, the obtained **Shapley values** would correspond to the **expected marginal contribution** of the feature value to the model's prediction. This expected marginal contribution reflects how the model prediction outcome deviated from the mean or naive prediction due to the observed feature value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde21787",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945f1306",
   "metadata": {},
   "source": [
    "To use Shapley values in explaining the model outcome for a single observation requires us to do several things: First, notice that [Equation 10](#mjx-eqn-eq%3Ashapley-math) necessiates us to **train different models** for all possible subsets of the features. Second, Shapley value is by nature a local explanation method. Thus, after model training we need to **select an instance** we want to explain then get the model prediction on that instance. Finally, equipped with the model prediction, we can **compute for the Shapley value** to quantify the contribution of each feature for that instance.\n",
    "\n",
    "In this analysis we use the [Boston Housing Dataset](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html), but we limit our *descriptive features* to three variables (for simplicity):\n",
    "\n",
    "**Descriptive Features**\n",
    "\n",
    "1. `DIS` - weighted distances to five Boston employment centres.\n",
    "2. `LSTAT` - % lower status of the population.\n",
    "3. `RM` - average number of rooms per dwelling.\n",
    "\n",
    "**Target Feature**\n",
    "\n",
    "4. `MEDV` - Median value of owner-occupied homes in $1000's.\n",
    "\n",
    "\n",
    "Let's start our implementation by importing the required default libraries and setting some preambles for prettification of our plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87e50b28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T11:40:25.262878Z",
     "start_time": "2021-10-13T11:40:23.579974Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(1337)\n",
    "\n",
    "# Some preambles for prettification\n",
    "warnings.filterwarnings('ignore')\n",
    "rcParams.update({'figure.figsize': (8, 6), 'axes.spines.top': False,\n",
    "                 'axes.spines.right': False, 'axes.labelsize': 14,\n",
    "                 'axes.titlesize': 16, 'axes.titleweight': 'bold'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57eff40",
   "metadata": {},
   "source": [
    "#### Step 0: Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d7670b",
   "metadata": {},
   "source": [
    "The Boston dataset is conveniently stored in the [`scikit-learn`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html) dataset. We load this dataset onto a dataframe, selecitng only the three features we specified earlier.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Deprecation Warning**\n",
    "\n",
    "The Boston dataset will be deprecated in `sklearn.dataset` in a future version.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d878ecb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T11:40:26.595073Z",
     "start_time": "2021-10-13T11:40:25.262878Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import dataset loader from sklearn.datasets\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "# Load dataset\n",
    "boston_dataset = load_boston()\n",
    "\n",
    "# Get predictors and target, use only these three columns for demo\n",
    "use_cols = ['RM', 'LSTAT', 'DIS']\n",
    "predictors = (pd.DataFrame(\n",
    "        boston_dataset['data'], columns=boston_dataset['feature_names']))\n",
    "predictors = predictors.loc[:, use_cols]\n",
    "target = boston_dataset['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f95db4",
   "metadata": {},
   "source": [
    "Let's preview the predictors and target to see that we've correctly loaded the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf9f1147",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T11:40:26.625073Z",
     "start_time": "2021-10-13T11:40:26.596076Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RM</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>DIS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.575</td>\n",
       "      <td>4.98</td>\n",
       "      <td>4.0900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.421</td>\n",
       "      <td>9.14</td>\n",
       "      <td>4.9671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.185</td>\n",
       "      <td>4.03</td>\n",
       "      <td>4.9671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.998</td>\n",
       "      <td>2.94</td>\n",
       "      <td>6.0622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.147</td>\n",
       "      <td>5.33</td>\n",
       "      <td>6.0622</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      RM  LSTAT     DIS\n",
       "0  6.575   4.98  4.0900\n",
       "1  6.421   9.14  4.9671\n",
       "2  7.185   4.03  4.9671\n",
       "3  6.998   2.94  6.0622\n",
       "4  7.147   5.33  6.0622"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db86bacf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T11:40:26.640074Z",
     "start_time": "2021-10-13T11:40:26.628078Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24. , 21.6, 34.7, 33.4, 36.2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2daea8",
   "metadata": {},
   "source": [
    "Finally, we segregate the dataset according to training and testing datasets in preparation for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8804eb06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T11:40:26.717074Z",
     "start_time": "2021-10-13T11:40:26.642078Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Perform train-test splitting\n",
    "train_predictors, test_predictors, train_targets, test_targets = (\n",
    "    train_test_split(predictors, target, test_size=0.20, random_state=1337))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ed0af0",
   "metadata": {},
   "source": [
    "#### Step 1: Training the model for each feature subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05813342",
   "metadata": {},
   "source": [
    "Now that we have our predictors and targets, the next step is to get all the possible feature subsets that we will train our models from. Here, we use the `combinations` function from the `itertools` library to get the different possible combinations of feature subsets, then use `chain.from_iterable()` to flatten the resulting list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57f9229c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T11:40:26.733077Z",
     "start_time": "2021-10-13T11:40:26.718077Z"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import combinations, chain\n",
    "\n",
    "# Get list of all possible non-empty subsets\n",
    "subsets_list = list(chain.from_iterable(\n",
    "    [combinations(sorted(predictors.columns), i) for i in range(1, 4)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1149b06",
   "metadata": {},
   "source": [
    "Verify that what we got all the possible non-null subsets of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ff702b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T11:40:26.749075Z",
     "start_time": "2021-10-13T11:40:26.735074Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('DIS',),\n",
       " ('LSTAT',),\n",
       " ('RM',),\n",
       " ('DIS', 'LSTAT'),\n",
       " ('DIS', 'RM'),\n",
       " ('LSTAT', 'RM'),\n",
       " ('DIS', 'LSTAT', 'RM')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsets_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b76528a",
   "metadata": {},
   "source": [
    "Next, we will for demonstration purposes, we will use a **Random Forest Regressor** as our model. We will no longer hypertune the model, but we will only train it using the training dataset. Notice also here that we iterate through the different feature subsets and train a model for each feature subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8495bed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T11:40:27.794023Z",
     "start_time": "2021-10-13T11:40:26.751076Z"
    }
   },
   "outputs": [],
   "source": [
    "# We will use RandomForestRegressor as our base model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Initialize model result\n",
    "models = {}\n",
    "\n",
    "# Iterate through subsets then fit the model on the feature subset\n",
    "for subset in subsets_list:\n",
    "    model = RandomForestRegressor(random_state=1337)\n",
    "    model.fit(train_predictors.loc[:, subset], train_targets)\n",
    "    models[subset] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2deb4df4",
   "metadata": {},
   "source": [
    "Now, let's verify that indeed we got and trained different models on different feature subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74543255",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T11:40:27.810023Z",
     "start_time": "2021-10-13T11:40:27.796024Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('DIS',): RandomForestRegressor(random_state=1337),\n",
       " ('LSTAT',): RandomForestRegressor(random_state=1337),\n",
       " ('RM',): RandomForestRegressor(random_state=1337),\n",
       " ('DIS', 'LSTAT'): RandomForestRegressor(random_state=1337),\n",
       " ('DIS', 'RM'): RandomForestRegressor(random_state=1337),\n",
       " ('LSTAT', 'RM'): RandomForestRegressor(random_state=1337),\n",
       " ('DIS', 'LSTAT', 'RM'): RandomForestRegressor(random_state=1337)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0ca304",
   "metadata": {},
   "source": [
    "#### Step 2: Getting the Shapley value for a sample test dataset instance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcd5c9c",
   "metadata": {},
   "source": [
    "Remember that Shapley value is naturally a local explainability method. As such, we need to select a sample test data point that we want to explain in particular. We arbitrarily set this as the first data point of the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8eddfb0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T11:40:27.825027Z",
     "start_time": "2021-10-13T11:40:27.812026Z"
    }
   },
   "outputs": [],
   "source": [
    "# Select a sample test data point\n",
    "sample_test = test_predictors.iloc[[0], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daaf254",
   "metadata": {},
   "source": [
    "Let's preview the sample test set that we got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45fa798a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T11:40:27.840024Z",
     "start_time": "2021-10-13T11:40:27.827028Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RM</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>DIS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>7.148</td>\n",
       "      <td>3.56</td>\n",
       "      <td>5.1167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RM  LSTAT     DIS\n",
       "291  7.148   3.56  5.1167"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a912906f",
   "metadata": {},
   "source": [
    "Now, let's get the predictions of all of our trained models on this sample test set. Note here that we set the *naive* predictioin as the mean of the targets in the training dataset. This would represent the model prediction when we don't use any features from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27519c70",
   "metadata": {},
   "source": [
    "<a name='cell:model-outcomes'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7f98e2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T11:40:27.918026Z",
     "start_time": "2021-10-13T11:40:27.841025Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize model outcome result, null prediction is naive prediction\n",
    "model_outcomes = {(): round(train_targets.mean(), 2)}\n",
    "\n",
    "# Iterate through all subsets\n",
    "for subset in subsets_list:\n",
    "    # Get the model prediction for each subset\n",
    "    model_outcomes[subset]= round(\n",
    "        models[subset].predict(sample_test.loc[:, subset])[0], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1598b0",
   "metadata": {},
   "source": [
    "Verify that we got a correct model predictions from each feature subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26fa0bdc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T11:40:27.934023Z",
     "start_time": "2021-10-13T11:40:27.920060Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(): 22.68,\n",
       " ('DIS',): 29.64,\n",
       " ('LSTAT',): 41.72,\n",
       " ('RM',): 35.93,\n",
       " ('DIS', 'LSTAT'): 34.19,\n",
       " ('DIS', 'RM'): 35.09,\n",
       " ('LSTAT', 'RM'): 36.39,\n",
       " ('DIS', 'LSTAT', 'RM'): 35.73}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b12803e",
   "metadata": {},
   "source": [
    "We see here that the model prediction for this instance yields `35.73` which is **higher** than the naive prediction. At this point, we want to explain why our model gave that prediction by finding the contribution of each feature using Shapley value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b65271e",
   "metadata": {},
   "source": [
    "#### Step 3: Compute the Shapley values from the model outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9511f107",
   "metadata": {},
   "source": [
    "From the results of **Step 2**, we got the outcomes or predictions of each of the model trained on different feature subsets. We can construct a network plot similar to [Figure 6](#fig:dragon-quest) to visualize the marginal contribution of each feature (see Figure [7](#fig:model-outcomes))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8449efd",
   "metadata": {},
   "source": [
    "<a name='fig:model-outcomes'></a>\n",
    "<div>\n",
    "    <img src=\"images/model-outcomes.png\" align=\"center\" width=\"450\"/>\n",
    "</div>\n",
    "\n",
    "<br style=\"clear:both\" />\n",
    "\n",
    "<div>\n",
    "    <p style=\"font-size:12px;font-style:default;\">\n",
    "        <b>Figure 7. Different possible feature subsets and the corresponding model outcome for each feature subset.</b><br>\n",
    "           Each arrow represents a feature value entering the coalition. Blue arrows represent the `DIS` feature joining the feature subset, pink arrows represent the `LSTAT` feature, and the purple arrows represent the `RM` feature.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906f3793",
   "metadata": {},
   "source": [
    "From the get-go, we see that including the `LSTAT` feature increases the model prediction substantially. We can  compute the Shapley value to confirm our intuition in the same way that we did. In this case, we codified the computation of the Shapley values into a helper function [`get_shapley_values`](../helper_codes/shapley_values.py). This takes in a dictionary in the form of the `model_outcomes` then outputs a dictionary containing the computed Shapley values of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93adffd0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T11:40:27.950026Z",
     "start_time": "2021-10-13T11:40:27.936026Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Uncomment this cell if you're using Google colab\n",
    "# !pip install git+https://github.com/leolorenzoii/ml2_interpretability.git#egg=helper_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29004bce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T11:40:27.981026Z",
     "start_time": "2021-10-13T11:40:27.951038Z"
    }
   },
   "outputs": [],
   "source": [
    "from helper_codes import get_shapley_values\n",
    "\n",
    "# Compute Shapley value using helper function.\n",
    "# Verify that our answer would be the same if we compute this manually.\n",
    "shapley_values = get_shapley_values(model_outcomes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b87d6de",
   "metadata": {},
   "source": [
    "Verify that we got the correct format for the result of the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3da0787d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T11:40:27.997029Z",
     "start_time": "2021-10-13T11:40:27.983042Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DIS': 0.7049999999999996, 'LSTAT': 7.394999999999997, 'RM': 4.95}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shapley_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a94337",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Practice Exercise**\n",
    "\n",
    "Verify that you get the same result if you were to compute the Shapley values manually. [Figure 7](#fig:model-outcomes) would be helpful in your computation.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5254afa",
   "metadata": {},
   "source": [
    "Finally, we plot the computed Shapley values as a bar plot for better interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dccf0564",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T11:40:28.678095Z",
     "start_time": "2021-10-13T11:40:27.999082Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = pd.Series(shapley_values).sort_values().plot(kind='barh')\n",
    "\n",
    "ax.set_xlabel(\"Shapley Value\")\n",
    "ax.set_ylabel(\"Feature\")\n",
    "ax.set_title(\"Sample test instance Shapley values\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5da279",
   "metadata": {},
   "source": [
    "Notice that the computed Shapley values aligns with our intuition. `LSTAT` has the highest contribution based on Shapley values. Do note that `LSTAT` refers to % of the population belonging to the lower status. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cc746e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Points for Discussion**\n",
    "    \n",
    "The plot showing the Shapley Value for each feature looks similar to a tree feature importance plot. Can you enumerate the differences between the tree feature importance plot and this feature Shapley value plot?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24229ef",
   "metadata": {},
   "source": [
    "### Advantages and Disadvantages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7218c0fa",
   "metadata": {},
   "source": [
    "Shapley values although is quite a popular method still has its limitations. Let us highlight in this final subsection its advantages and disadvantages. *For more deeper discussion please see Chapter [9.5.4](https://christophm.github.io/interpretable-ml-book/shapley.html#advantages-16) and [9.5.5](https://christophm.github.io/interpretable-ml-book/shapley.html#disadvantages-16)*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01df830",
   "metadata": {},
   "source": [
    "#### Advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e37c1a",
   "metadata": {},
   "source": [
    "1. Shapley values is grounded on a **solid theory** which helps distribute the effects fairly. This means that for regulatory requirements such as EU's \"right to explanations\", Shapley value might be the legally complaint method.\n",
    "2. Shapley values provide **contrastive explanations**. You can compare the result to a subset or a single data point. Aggregating the result for different instances also allow us to compare how the magnitude of the feature value affects the Shapley value.\n",
    "3. Although not discussed here, **Shapley values have several axioms** which are: efficiency, symmetry, dummy, and addivity. These axioms can be used to extend how Shapley values may be used, such as adding each values to make the local explanation become global explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2fda5e",
   "metadata": {},
   "source": [
    "#### Disadvantages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f814336",
   "metadata": {},
   "source": [
    "1. The main disadvantage of Shapley values is **computational complexity**. Notice that we needed to train a model for each feature subset here. As such, as the features grow in size, the possible coalitions grows exponentially.\n",
    "2. Shapley values is prone to **misinterpretations**. Shapley value is NOT the difference of the model outcome after removing the feature from model training. It is the contribution of a feature value to the difference between the actual prediction and the naive prediction of the model.\n",
    "3. Shapley values **cannot be used to hypothesize** changes in the prediction if feature values is changed for a particular instance in your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9dd8e9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "In this demonstration, we showed how we can compute for the Shapley value of each feature value for a sample test dataset instance. Replicate this procedure to get the Shapley values for **ALL** test dataset instances. Get the **mean** Shapley value of each feature then interpret the result. Compare the mean Shapley value of each features with the tree feature importance (obtained from the model containing all the features).\n",
    "\n",
    "\n",
    "**Steps**\n",
    "    \n",
    "1. Modify [this cell](#cell:model-outcomes) to predict on the *test* dataset instead of just a sample instance. You will should get a dictionary with keys as feature subset and values as array of predictions.\n",
    "2. Iterate through the values of the dictionary then get the Shapley value of each feature for each test sample instance using the `get_shapley_values()` function. Record the result as a list of dictionary of Shapley values for each feature.\n",
    "3. Construct a dataframe using the result of step 2, then get the mean Shapley values.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6088ddb7",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccff2fa",
   "metadata": {},
   "source": [
    "<a name='ref:interpret-ml'></a> [1] Guo, Mengzhuo, et al. \"An interpretable machine learning framework for modelling human decision behavior.\" *arXiv preprint arXiv:1906.01233* (2019).\n",
    "\n",
    "<a name='ref:molnar'></a> [2] Molnar, Christoph. â€œInterpretable machine learning. A Guide for Making Black Box Models Explainableâ€, 2019. https://christophm.github.io/interpretable-ml-book/.\n",
    "\n",
    "<a name='ref:miller'></a> [3] Miller, Tim. â€œExplanation in artificial intelligence: Insights from the social sciences.â€ *arXiv Preprint arXiv:1706.07269.* (2017)\n",
    "\n",
    "<a name='ref:kim'></a> [4] Kim, Been, Rajiv Khanna, and Oluwasanmi O. Koyejo. â€œExamples are not enough, learn to criticize! Criticism for interpretability.â€ *Advances in Neural Information Processing Systems* (2016).\n",
    "\n",
    "<a name='ref:crisp-dm'></a> [5] Kelleher, John D., Brian Mac Namee, and Aoife Dâ€™Arcy. \"Machine Learning for Predictive Analytics: The Predictive Data Analytics Project Lifecycle: CRISP-DM.\" *Fundamentals of machine learning for predictive analytics*, The MIT Press, 2020, pp. 15-17.\n",
    "\n",
    "<a name='ref:great-ai-debate'></a> [6] NeurIPS 2017. â€œThe Great AI Debate - NIPS2017 - Yann LeCun.â€ *YouTube*, uploaded by The Artificial Intelligence Channel, 1 February 2018, https://youtu.be/93Xv8vJ2acI.\n",
    "\n",
    "<a name='ref:bbox-peek'></a> [7] Adadi, Amina, and Mohammed Berrada. \"Peeking inside the black-box: a survey on explainable artificial intelligence (XAI).\" *IEEE access 6* (2018): 52138-52160.\n",
    "\n",
    "<a name='ref:rulefit'></a> [8] Friedman, Jerome H., and Bogdan E. Popescu. \"Predictive learning via rule ensembles.\" The Annals of Applied Statistics 2.3 (2008): 916-954.\n",
    "\n",
    "<a name='ref:med-image'></a> [9] Singh, Amitojdeep, Sourya Sengupta, and Vasudevan Lakshminarayanan. \"Explainable deep learning models in medical image analysis.\" *Journal of Imaging 6.6* (2020): 52.\n",
    "\n",
    "<a name='ref:shap-how'></a> [10] Mazzanti, Samuele. \"SHAP Values Explained Exactly How You Wished Someone Explained to You.\" *Towards Data Science*, 04 Apr. 2020, https://towardsdatascience.com/shap-explained-the-way-i-wish-someone-explained-it-to-me-ab81cc69ef30\n",
    "\n",
    "<a name='ref:shapley-handbook'></a> [11] Algaba, EncarnaciÃ³n, Vito Fragnelli, and JoaquÃ­n SÃ¡nchez-Soriano, eds. *Handbook of the Shapley value*. CRC Press, 2019.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
